**<h1 align="center">Learn Data Processing with Spark SQL using PySpark on AWS</h1>**

![Untitled](https://user-images.githubusercontent.com/107995802/180428326-8814cfea-2b0d-4372-b427-128edc46f780.png)

## Agenda
In this AWS Spark SQL project, you will analyze the Movies and Ratings Dataset using RDD and Spark SQL to get hands-on experience on the fundamentals of Scala programming language.

## Tech Stack

➔ AWS EC2, Docker, Putty, Spark

### Docker
Docker is a free and open-source containerization platform, and it enables programmers
to package programs into containers. These standardized executable components
combine application source code with the libraries and dependencies required to run
that code in any environment.

## Approach
● Create an AWS EC2 instance and launch it.

● Create docker images using docker-compose file on EC2 machine via ssh.

● Move the dataset into EC2 instance from local machine

● Move the dataset from EC2 instance to PySpark Directory using Docker

● Extract and Transform the data in PySpark using RDD and DataFrame

● Save the Output into PySpark Directory

● Move the Output Folder from PySpark to EC2 instance Home Directory using Docker

● Move the Output Folder EC2 instance Home Directory to Host Local Machine using Docker

## Project Takeaways
● Understanding various services provided by AWS

● Creating an AWS EC2 instance

● Connecting to an AWS EC2 instance via SSH

● Introduction to Docker

● Visualizing the complete Architecture of the system

● Usage of docker-composer and starting all tools

● Copying a file from a local machine to an EC2 machine

● Understanding the schema of the dataset

● Data ingestion/transformation using Sqoop, Spark, and Hive

● Moving the data from PySpark container to EC2 Instamce then move to the Local HOST MACHINE 
